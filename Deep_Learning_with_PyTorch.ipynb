{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPHT62T4XJEGpHv6eDrc2xE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariefpurnamamuharram/MyTransformerResearch/blob/master/Deep_Learning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch\n",
        "____\n",
        "YouTube videos:\n",
        "- https://www.youtube.com/watch?v=c36lUUr864M\n",
        "\n",
        "Basics of Tensor:\n",
        "- torch.add(x,y) --> Addition of x and y\n",
        "- torch.mul(x,y) --> Multiplication of x and y\n",
        "- torch.rand(n)\n",
        "- torch.sub(x,y) --> Substraction of x and y\n",
        "- torch.view(1) --> Reshaping Torch Tensor\n",
        "\n",
        "GPU functions:\n",
        "- torch.cuda.is_available() --> Check if CUDA is available"
      ],
      "metadata": {
        "id": "feWr5wWZ6wjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RX67buGiulC",
        "outputId": "9f3b0cf2-d488-4bc0-92d5-ae96d4907295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics of Tensor\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available(): # Check if CUDA is available\n",
        "  device = torch.device(\"cuda\") # Select CUDA device (ex: \"CUDA:0\")\n",
        "  x = torch.ones(5, device=device) # Move Tensor to the GPU\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # Move Tensor to the GPU\n",
        "  z = x + y # Processed at the GPU level\n",
        "  # z.numppy(), will produce an error. NumPy only can be processed at CPU level\n",
        "  z = z.to(\"CPU\") # Move the Tensor to the CPU"
      ],
      "metadata": {
        "id": "NvdqMkiX5-Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd\n",
        "# Gradient (Grad) is important in our model optimization\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "z = y*y*2\n",
        "#z = z.mean()\n",
        "print(z)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.01], dtype=torch.float32)\n",
        "z.backward(v) # dz/dx, Jacobian products\n",
        "print(x.grad)\n",
        "\n",
        "# ----\n",
        "# Stop PyTorch treating the gradient functions and tracking history in \n",
        "# our compational graphs.\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad()\n",
        "\n",
        "a = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(a)\n",
        "\n",
        "a.requires_grad_(False)\n",
        "print(a)\n",
        "\n",
        "# ---\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  # Clear it before the next epoch so the grad value is still right.\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGFyY7Le--Po",
        "outputId": "a5020c01-1372-4d82-a388-3a76b1b40c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3010, 0.5801, 0.3912], requires_grad=True)\n",
            "tensor([2.3010, 2.5801, 2.3912], grad_fn=<AddBackward0>)\n",
            "tensor([10.5891, 13.3141, 11.4357], grad_fn=<MulBackward0>)\n",
            "tensor([ 0.9204, 10.3205,  0.0956])\n",
            "tensor([0.2824, 0.7041, 0.6977], requires_grad=True)\n",
            "tensor([0.2824, 0.7041, 0.6977])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # Initial weight\n",
        "\n",
        "# Forward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss) # Print loss\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### Update weights\n",
        "### next forward and backwards\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRjNyOhNBYlV",
        "outputId": "c83c6cc1-9e3f-438c-9ae3-056585ba273f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Manual way, using NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "# Gradient\n",
        "# MSE = 1/N * (w*X - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoc % 1 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "PbOXUMb6FCq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a5eae5-9e00-4b1b-8caa-28d742810fe3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 1.2, loss = 30.00000000\n",
            "epoch 2: w = 1.68, loss = 4.79999924\n",
            "epoch 3: w = 1.87, loss = 0.76800019\n",
            "epoch 4: w = 1.95, loss = 0.12288000\n",
            "epoch 5: w = 1.98, loss = 0.01966083\n",
            "epoch 6: w = 1.99, loss = 0.00314574\n",
            "epoch 7: w = 2.0, loss = 0.00050331\n",
            "epoch 8: w = 2.0, loss = 0.00008053\n",
            "epoch 9: w = 2.0, loss = 0.00001288\n",
            "epoch 10: w = 2.0, loss = 0.00000206\n",
            "epoch 11: w = 2.0, loss = 0.00000033\n",
            "epoch 12: w = 2.0, loss = 0.00000005\n",
            "epoch 13: w = 2.0, loss = 0.00000001\n",
            "epoch 14: w = 2.0, loss = 0.00000000\n",
            "epoch 15: w = 2.0, loss = 0.00000000\n",
            "epoch 16: w = 2.0, loss = 0.00000000\n",
            "epoch 17: w = 2.0, loss = 0.00000000\n",
            "epoch 18: w = 2.0, loss = 0.00000000\n",
            "epoch 19: w = 2.0, loss = 0.00000000\n",
            "epoch 20: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Tensor way, using PyTorch\n",
        "\n",
        "import torch\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoc % 10 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPwbr1oIKn0f",
        "outputId": "98899168-8cdb-4d5b-8e84-49c0e6727ae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 0.3, loss = 30.00000000\n",
            "epoch 11: w = 1.67, loss = 1.16278565\n",
            "epoch 21: w = 1.93, loss = 0.04506890\n",
            "epoch 31: w = 1.99, loss = 0.00174685\n",
            "epoch 41: w = 2.0, loss = 0.00006770\n",
            "epoch 51: w = 2.0, loss = 0.00000262\n",
            "epoch 61: w = 2.0, loss = 0.00000010\n",
            "epoch 71: w = 2.0, loss = 0.00000000\n",
            "epoch 81: w = 2.0, loss = 0.00000000\n",
            "epoch 91: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Pipeline: Model/Loss/Optimizer\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Rows as the number of the samples\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features) # 4 samples with 1 feature\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Predicted before training: f(5) = {model(X_test).item()}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Stochastic Gradient Descent (SGD)\n",
        "# model.parameters contains the weights\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoc % 100 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoc+1}: w = {w[0][0].item():.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {model(X_test).item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0It_9PdtC5",
        "outputId": "e72a266c-7d2e-4181-baa7-b39ba83333ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Predicted before training: f(5) = -3.603912353515625\n",
            "epoch 1: w = -0.318, loss = 55.37839508\n",
            "epoch 101: w = 1.79, loss = 0.06692869\n",
            "epoch 201: w = 1.84, loss = 0.03674300\n",
            "epoch 301: w = 1.88, loss = 0.02017150\n",
            "epoch 401: w = 1.91, loss = 0.01107389\n",
            "epoch 501: w = 1.94, loss = 0.00607944\n",
            "epoch 601: w = 1.95, loss = 0.00333753\n",
            "epoch 701: w = 1.96, loss = 0.00183227\n",
            "epoch 801: w = 1.97, loss = 0.00100590\n",
            "epoch 901: w = 1.98, loss = 0.00055222\n",
            "Predicted after training: f(5) = 9.970149040222168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) prepare data\n",
        "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
        "Y = Y.view(Y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# 1) model\n",
        "input_size = n_features\n",
        "ouput_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass and loss\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted, Y)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_numpy, Y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "rIhczOVN6zww",
        "outputId": "c1502862-bc59-400d-f2d1-1e2c57c73e9e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4436.9639\n",
            "epoch: 20, loss = 3308.0378\n",
            "epoch: 30, loss = 2491.6191\n",
            "epoch: 40, loss = 1900.5485\n",
            "epoch: 50, loss = 1472.1865\n",
            "epoch: 60, loss = 1161.4496\n",
            "epoch: 70, loss = 935.8409\n",
            "epoch: 80, loss = 771.9070\n",
            "epoch: 90, loss = 652.6998\n",
            "epoch: 100, loss = 565.9565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1I6B7ws99u-S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}