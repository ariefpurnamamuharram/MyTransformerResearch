{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5CfFUybBHCoQGTO739RB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariefpurnamamuharram/MyTransformerResearch/blob/master/Deep_Learning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch\n",
        "____\n",
        "YouTube videos:\n",
        "- https://www.youtube.com/watch?v=c36lUUr864M\n",
        "\n",
        "Basics of Tensor:\n",
        "- torch.add(x,y) --> Addition of x and y\n",
        "- torch.mul(x,y) --> Multiplication of x and y\n",
        "- torch.rand(n)\n",
        "- torch.sub(x,y) --> Substraction of x and y\n",
        "- torch.view(1) --> Reshaping Torch Tensor\n",
        "\n",
        "GPU functions:\n",
        "- torch.cuda.is_available() --> Check if CUDA is available"
      ],
      "metadata": {
        "id": "feWr5wWZ6wjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RX67buGiulC",
        "outputId": "b45c1964-007f-4a2f-a9dd-7d0ed5954c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics of Tensor\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available(): # Check if CUDA is available\n",
        "  device = torch.device(\"cuda\") # Select CUDA device (ex: \"CUDA:0\")\n",
        "  x = torch.ones(5, device=device) # Move Tensor to the GPU\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # Move Tensor to the GPU\n",
        "  z = x + y # Processed at the GPU level\n",
        "  # z.numppy(), will produce an error. NumPy only can be processed at CPU level\n",
        "  z = z.to(\"CPU\") # Move the Tensor to the CPU"
      ],
      "metadata": {
        "id": "NvdqMkiX5-Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd\n",
        "# Gradient (Grad) is important in our model optimization\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "z = y*y*2\n",
        "#z = z.mean()\n",
        "print(z)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.01], dtype=torch.float32)\n",
        "z.backward(v) # dz/dx, Jacobian products\n",
        "print(x.grad)\n",
        "\n",
        "# ----\n",
        "# Stop PyTorch treating the gradient functions and tracking history in \n",
        "# our compational graphs.\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad()\n",
        "\n",
        "a = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(a)\n",
        "\n",
        "a.requires_grad_(False)\n",
        "print(a)\n",
        "\n",
        "# ---\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  # Clear it before the next epoch so the grad value is still right.\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGFyY7Le--Po",
        "outputId": "a5020c01-1372-4d82-a388-3a76b1b40c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3010, 0.5801, 0.3912], requires_grad=True)\n",
            "tensor([2.3010, 2.5801, 2.3912], grad_fn=<AddBackward0>)\n",
            "tensor([10.5891, 13.3141, 11.4357], grad_fn=<MulBackward0>)\n",
            "tensor([ 0.9204, 10.3205,  0.0956])\n",
            "tensor([0.2824, 0.7041, 0.6977], requires_grad=True)\n",
            "tensor([0.2824, 0.7041, 0.6977])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # Initial weight\n",
        "\n",
        "# Forward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss) # Print loss\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### Update weights\n",
        "### next forward and backwards\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRjNyOhNBYlV",
        "outputId": "c83c6cc1-9e3f-438c-9ae3-056585ba273f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Manual way, using NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "# Gradient\n",
        "# MSE = 1/N * (w*X - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoc % 1 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "PbOXUMb6FCq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a5eae5-9e00-4b1b-8caa-28d742810fe3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 1.2, loss = 30.00000000\n",
            "epoch 2: w = 1.68, loss = 4.79999924\n",
            "epoch 3: w = 1.87, loss = 0.76800019\n",
            "epoch 4: w = 1.95, loss = 0.12288000\n",
            "epoch 5: w = 1.98, loss = 0.01966083\n",
            "epoch 6: w = 1.99, loss = 0.00314574\n",
            "epoch 7: w = 2.0, loss = 0.00050331\n",
            "epoch 8: w = 2.0, loss = 0.00008053\n",
            "epoch 9: w = 2.0, loss = 0.00001288\n",
            "epoch 10: w = 2.0, loss = 0.00000206\n",
            "epoch 11: w = 2.0, loss = 0.00000033\n",
            "epoch 12: w = 2.0, loss = 0.00000005\n",
            "epoch 13: w = 2.0, loss = 0.00000001\n",
            "epoch 14: w = 2.0, loss = 0.00000000\n",
            "epoch 15: w = 2.0, loss = 0.00000000\n",
            "epoch 16: w = 2.0, loss = 0.00000000\n",
            "epoch 17: w = 2.0, loss = 0.00000000\n",
            "epoch 18: w = 2.0, loss = 0.00000000\n",
            "epoch 19: w = 2.0, loss = 0.00000000\n",
            "epoch 20: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Tensor way, using PyTorch\n",
        "\n",
        "import torch\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoc % 10 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPwbr1oIKn0f",
        "outputId": "98899168-8cdb-4d5b-8e84-49c0e6727ae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 0.3, loss = 30.00000000\n",
            "epoch 11: w = 1.67, loss = 1.16278565\n",
            "epoch 21: w = 1.93, loss = 0.04506890\n",
            "epoch 31: w = 1.99, loss = 0.00174685\n",
            "epoch 41: w = 2.0, loss = 0.00006770\n",
            "epoch 51: w = 2.0, loss = 0.00000262\n",
            "epoch 61: w = 2.0, loss = 0.00000010\n",
            "epoch 71: w = 2.0, loss = 0.00000000\n",
            "epoch 81: w = 2.0, loss = 0.00000000\n",
            "epoch 91: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Pipeline: Model/Loss/Optimizer\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Rows as the number of the samples\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features) # 4 samples with 1 feature\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Predicted before training: f(5) = {model(X_test).item()}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Stochastic Gradient Descent (SGD)\n",
        "# model.parameters contains the weights\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoc % 100 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoc+1}: w = {w[0][0].item():.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {model(X_test).item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0It_9PdtC5",
        "outputId": "e72a266c-7d2e-4181-baa7-b39ba83333ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Predicted before training: f(5) = -3.603912353515625\n",
            "epoch 1: w = -0.318, loss = 55.37839508\n",
            "epoch 101: w = 1.79, loss = 0.06692869\n",
            "epoch 201: w = 1.84, loss = 0.03674300\n",
            "epoch 301: w = 1.88, loss = 0.02017150\n",
            "epoch 401: w = 1.91, loss = 0.01107389\n",
            "epoch 501: w = 1.94, loss = 0.00607944\n",
            "epoch 601: w = 1.95, loss = 0.00333753\n",
            "epoch 701: w = 1.96, loss = 0.00183227\n",
            "epoch 801: w = 1.97, loss = 0.00100590\n",
            "epoch 901: w = 1.98, loss = 0.00055222\n",
            "Predicted after training: f(5) = 9.970149040222168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rIhczOVN6zww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}