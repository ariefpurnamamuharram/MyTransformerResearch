{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKq/2drOBm9nr29F+6ny51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariefpurnamamuharram/MyTransformerResearch/blob/master/Deep_Learning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch\n",
        "____\n",
        "YouTube videos:\n",
        "- https://www.youtube.com/watch?v=c36lUUr864M\n",
        "\n",
        "Basics of Tensor:\n",
        "- torch.add(x,y) --> Addition of x and y\n",
        "- torch.mul(x,y) --> Multiplication of x and y\n",
        "- torch.rand(n)\n",
        "- torch.sub(x,y) --> Substraction of x and y\n",
        "- torch.view(1) --> Reshaping Torch Tensor\n",
        "\n",
        "GPU functions:\n",
        "- torch.cuda.is_available() --> Check if CUDA is available"
      ],
      "metadata": {
        "id": "feWr5wWZ6wjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RX67buGiulC",
        "outputId": "9f3b0cf2-d488-4bc0-92d5-ae96d4907295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics of Tensor\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available(): # Check if CUDA is available\n",
        "  device = torch.device(\"cuda\") # Select CUDA device (ex: \"CUDA:0\")\n",
        "  x = torch.ones(5, device=device) # Move Tensor to the GPU\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # Move Tensor to the GPU\n",
        "  z = x + y # Processed at the GPU level\n",
        "  # z.numppy(), will produce an error. NumPy only can be processed at CPU level\n",
        "  z = z.to(\"CPU\") # Move the Tensor to the CPU"
      ],
      "metadata": {
        "id": "NvdqMkiX5-Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd\n",
        "# Gradient (Grad) is important in our model optimization\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "z = y*y*2\n",
        "#z = z.mean()\n",
        "print(z)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.01], dtype=torch.float32)\n",
        "z.backward(v) # dz/dx, Jacobian products\n",
        "print(x.grad)\n",
        "\n",
        "# ----\n",
        "# Stop PyTorch treating the gradient functions and tracking history in \n",
        "# our compational graphs.\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad()\n",
        "\n",
        "a = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(a)\n",
        "\n",
        "a.requires_grad_(False)\n",
        "print(a)\n",
        "\n",
        "# ---\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  # Clear it before the next epoch so the grad value is still right.\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGFyY7Le--Po",
        "outputId": "a5020c01-1372-4d82-a388-3a76b1b40c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3010, 0.5801, 0.3912], requires_grad=True)\n",
            "tensor([2.3010, 2.5801, 2.3912], grad_fn=<AddBackward0>)\n",
            "tensor([10.5891, 13.3141, 11.4357], grad_fn=<MulBackward0>)\n",
            "tensor([ 0.9204, 10.3205,  0.0956])\n",
            "tensor([0.2824, 0.7041, 0.6977], requires_grad=True)\n",
            "tensor([0.2824, 0.7041, 0.6977])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # Initial weight\n",
        "\n",
        "# Forward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss) # Print loss\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### Update weights\n",
        "### next forward and backwards\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRjNyOhNBYlV",
        "outputId": "c83c6cc1-9e3f-438c-9ae3-056585ba273f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Manual way, using NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "# Gradient\n",
        "# MSE = 1/N * (w*X - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoc % 1 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "PbOXUMb6FCq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a5eae5-9e00-4b1b-8caa-28d742810fe3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 1.2, loss = 30.00000000\n",
            "epoch 2: w = 1.68, loss = 4.79999924\n",
            "epoch 3: w = 1.87, loss = 0.76800019\n",
            "epoch 4: w = 1.95, loss = 0.12288000\n",
            "epoch 5: w = 1.98, loss = 0.01966083\n",
            "epoch 6: w = 1.99, loss = 0.00314574\n",
            "epoch 7: w = 2.0, loss = 0.00050331\n",
            "epoch 8: w = 2.0, loss = 0.00008053\n",
            "epoch 9: w = 2.0, loss = 0.00001288\n",
            "epoch 10: w = 2.0, loss = 0.00000206\n",
            "epoch 11: w = 2.0, loss = 0.00000033\n",
            "epoch 12: w = 2.0, loss = 0.00000005\n",
            "epoch 13: w = 2.0, loss = 0.00000001\n",
            "epoch 14: w = 2.0, loss = 0.00000000\n",
            "epoch 15: w = 2.0, loss = 0.00000000\n",
            "epoch 16: w = 2.0, loss = 0.00000000\n",
            "epoch 17: w = 2.0, loss = 0.00000000\n",
            "epoch 18: w = 2.0, loss = 0.00000000\n",
            "epoch 19: w = 2.0, loss = 0.00000000\n",
            "epoch 20: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Tensor way, using PyTorch\n",
        "\n",
        "import torch\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoc % 10 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPwbr1oIKn0f",
        "outputId": "98899168-8cdb-4d5b-8e84-49c0e6727ae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 0.3, loss = 30.00000000\n",
            "epoch 11: w = 1.67, loss = 1.16278565\n",
            "epoch 21: w = 1.93, loss = 0.04506890\n",
            "epoch 31: w = 1.99, loss = 0.00174685\n",
            "epoch 41: w = 2.0, loss = 0.00006770\n",
            "epoch 51: w = 2.0, loss = 0.00000262\n",
            "epoch 61: w = 2.0, loss = 0.00000010\n",
            "epoch 71: w = 2.0, loss = 0.00000000\n",
            "epoch 81: w = 2.0, loss = 0.00000000\n",
            "epoch 91: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Pipeline: Model/Loss/Optimizer\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Rows as the number of the samples\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features) # 4 samples with 1 feature\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Predicted before training: f(5) = {model(X_test).item()}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Stochastic Gradient Descent (SGD)\n",
        "# model.parameters contains the weights\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoc % 100 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoc+1}: w = {w[0][0].item():.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {model(X_test).item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0It_9PdtC5",
        "outputId": "e72a266c-7d2e-4181-baa7-b39ba83333ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Predicted before training: f(5) = -3.603912353515625\n",
            "epoch 1: w = -0.318, loss = 55.37839508\n",
            "epoch 101: w = 1.79, loss = 0.06692869\n",
            "epoch 201: w = 1.84, loss = 0.03674300\n",
            "epoch 301: w = 1.88, loss = 0.02017150\n",
            "epoch 401: w = 1.91, loss = 0.01107389\n",
            "epoch 501: w = 1.94, loss = 0.00607944\n",
            "epoch 601: w = 1.95, loss = 0.00333753\n",
            "epoch 701: w = 1.96, loss = 0.00183227\n",
            "epoch 801: w = 1.97, loss = 0.00100590\n",
            "epoch 901: w = 1.98, loss = 0.00055222\n",
            "Predicted after training: f(5) = 9.970149040222168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) prepare data\n",
        "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
        "Y = Y.view(Y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# 1) model\n",
        "input_size = n_features\n",
        "ouput_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass and loss\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted, Y)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_numpy, Y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "rIhczOVN6zww",
        "outputId": "c1502862-bc59-400d-f2d1-1e2c57c73e9e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4436.9639\n",
            "epoch: 20, loss = 3308.0378\n",
            "epoch: 30, loss = 2491.6191\n",
            "epoch: 40, loss = 1900.5485\n",
            "epoch: 50, loss = 1472.1865\n",
            "epoch: 60, loss = 1161.4496\n",
            "epoch: 70, loss = 935.8409\n",
            "epoch: 80, loss = 771.9070\n",
            "epoch: 90, loss = 652.6998\n",
            "epoch: 100, loss = 565.9565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Convert to tensor\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "Y_train = torch.from_numpy(Y_train.astype(np.float32))\n",
        "Y_test = torch.from_numpy(Y_test.astype(np.float32))\n",
        "\n",
        "y_train = Y_train.view(Y_train.shape[0], 1)\n",
        "y_test = Y_test.view(Y_test.shape[0], 1)\n",
        "\n",
        "# 1) model\n",
        "# f = wx + b, sigmoid at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "# BCELoss: Binary Cross Entropy Loss\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass and loss\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # updates\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "  acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'accuracy = {acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I6B7ws99u-S",
        "outputId": "cd1af114-6c1f-4a50-f89b-4eb207a33621"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569 30\n",
            "epoch: 10, loss = 0.6803\n",
            "epoch: 20, loss = 0.5225\n",
            "epoch: 30, loss = 0.4331\n",
            "epoch: 40, loss = 0.3765\n",
            "epoch: 50, loss = 0.3372\n",
            "epoch: 60, loss = 0.3081\n",
            "epoch: 70, loss = 0.2854\n",
            "epoch: 80, loss = 0.2672\n",
            "epoch: 90, loss = 0.2522\n",
            "epoch: 100, loss = 0.2395\n",
            "accuracy = 0.9298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset & DataLoader\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset):\n",
        "    # data loading\n",
        "    xy = np.loadtxt(dataset, delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "    self.x = torch.from_numpy(xy[:, 1:])\n",
        "    self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1\n",
        "    self.n_samples = xy.shape[0]\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    # dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "WINE_DATASET_CSV = 'https://raw.githubusercontent.com/python-engineer/pytorchTutorial/master/data/wine/wine.csv'\n",
        "\n",
        "dataset = WineDataset(WINE_DATASET_CSV)\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples / 4)\n",
        "print(total_samples, n_iterations)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (inputs, labels) in enumerate(dataloader):\n",
        "    # forward backward, update\n",
        "    if (i+1) % 5 == 0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IwDHpccBX_Z",
        "outputId": "e2ca581e-381c-49f2-9b41-099c40744603"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178 45\n",
            "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
            "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Transforms\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset, transform=None):\n",
        "    # data loading\n",
        "    xy = np.loadtxt(dataset, delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "    self.x = xy[:, 1:]\n",
        "    self.y = xy[:, [0]]\n",
        "\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    # dataset[0]\n",
        "    sample = self.x[index], self.y[index]\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    \n",
        "    return sample\n",
        "  \n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "class ToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MulTransform:\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    inputs, target = sample\n",
        "    inputs *= self.factor\n",
        "    return inputs, target\n",
        "\n",
        "WINE_DATASET_CSV_URL = 'https://raw.githubusercontent.com/python-engineer/pytorchTutorial/master/data/wine/wine.csv'\n",
        "\n",
        "dataset = WineDataset(dataset=WINE_DATASET_CSV_URL, transform=ToTensor())\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
        "dataset = WineDataset(dataset=WINE_DATASET_CSV_URL, transform=composed)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF2dSjnLGqeK",
        "outputId": "c3b7e75e-e7c9-4a86-8763-453069b3e19c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
            "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
            "        4.2600e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax & Cross Entropy\n",
        "# ----\n",
        "# Softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:', outputs)\n",
        "\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "outputs = torch.softmax(x, dim=0)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4lQZxfmLO61",
        "outputId": "30f6deb3-892d-41d5-e6cb-e130a2323748"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax & Cross Entropy\n",
        "# ----\n",
        "# CrossEntropyLoss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3 samples\n",
        "Y = torch.tensor([2, 0, 1])\n",
        "\n",
        "# nsamples x nclasses = 3x3\n",
        "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1],[2.0, 1.0, 0.1],[0.1, 3.0, 0.1]])\n",
        "Y_pred_bad = torch.tensor([[2.1, 2.0, 0.1],[0.1, 1.0, 2.1],[0.1, 3.0, 0.1]])\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good, 1)\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUalGoUiT6PJ",
        "outputId": "f9b4c5d3-76de-4fbf-a4d0-8baeb8fe0a82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3018244206905365\n",
            "1.7338258028030396\n",
            "tensor([2, 0, 1])\n",
            "tensor([0, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Functions\n",
        "# -----------------------\n",
        "# Most popular activation functions\n",
        "# 1. Step functions -> Not used in practice\n",
        "# 2. Sigmoid --> Typically used in binary classification problem\n",
        "# 3. TanH --> Good choice in hidden layers\n",
        "# 4. ReLU --> If you don't know what to use, just use a ReLU for hidden layers\n",
        "# 5. Leaky ReLU --> Improved version of ReLU\n",
        "# 6. Softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# option 1 (create nn modules)\n",
        "class NeuralNet1(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet1, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.linear(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = torch.relu(self.linear1(x))\n",
        "    out = torch.sigmoid(self.linear2(x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "Aj6l7hdoswzs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed-Forward NeuralNet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# device config\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyper parameters\n",
        "input_size = 784 # 28x28, flattened into 1-D Tensor\n",
        "hidden_size = 100\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           download=True, \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor())\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size,\n",
        "                                           shuffle=False)\n",
        "\n",
        "examples = iter(train_loader)\n",
        "samples, labels = examples.next()\n",
        "print(samples.shape, labels.shape)\n",
        "\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.imshow(samples[i][0], cmap='gray')\n",
        "plt.show\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # 100, 1, 28, 28\n",
        "    # 100, 78\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # backwards\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "\n",
        "# test\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for images, labels in test_loader:\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(images)\n",
        "\n",
        "    # value, index\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    n_samples += labels.shape[0]\n",
        "    n_correct += (predictions == labels).sum().item()\n",
        "  \n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'accuracy = {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "QmoAdLi0xRa9",
        "outputId": "383c1cc9-723f-4525-acdd-fbcde69cc3d0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
            "epoch 1/2, step 100/600, loss = 0.4297\n",
            "epoch 1/2, step 200/600, loss = 0.3138\n",
            "epoch 1/2, step 300/600, loss = 0.2831\n",
            "epoch 1/2, step 400/600, loss = 0.4198\n",
            "epoch 1/2, step 500/600, loss = 0.2959\n",
            "epoch 1/2, step 600/600, loss = 0.1833\n",
            "epoch 2/2, step 100/600, loss = 0.2834\n",
            "epoch 2/2, step 200/600, loss = 0.1721\n",
            "epoch 2/2, step 300/600, loss = 0.4287\n",
            "epoch 2/2, step 400/600, loss = 0.1873\n",
            "epoch 2/2, step 500/600, loss = 0.1509\n",
            "epoch 2/2, step 600/600, loss = 0.2954\n",
            "accuracy = 9.78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3LaHd9uWzhHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}