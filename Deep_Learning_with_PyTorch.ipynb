{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiPOzaEPggVxYrl0G5iMSD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariefpurnamamuharram/MyTransformerResearch/blob/master/Deep_Learning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch\n",
        "____\n",
        "YouTube videos:\n",
        "- https://www.youtube.com/watch?v=c36lUUr864M\n",
        "\n",
        "Basics of Tensor:\n",
        "- torch.add(x,y) --> Addition of x and y\n",
        "- torch.mul(x,y) --> Multiplication of x and y\n",
        "- torch.rand(n)\n",
        "- torch.sub(x,y) --> Substraction of x and y\n",
        "- torch.view(1) --> Reshaping Torch Tensor\n",
        "\n",
        "GPU functions:\n",
        "- torch.cuda.is_available() --> Check if CUDA is available"
      ],
      "metadata": {
        "id": "feWr5wWZ6wjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RX67buGiulC",
        "outputId": "9f3b0cf2-d488-4bc0-92d5-ae96d4907295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics of Tensor\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available(): # Check if CUDA is available\n",
        "  device = torch.device(\"cuda\") # Select CUDA device (ex: \"CUDA:0\")\n",
        "  x = torch.ones(5, device=device) # Move Tensor to the GPU\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # Move Tensor to the GPU\n",
        "  z = x + y # Processed at the GPU level\n",
        "  # z.numppy(), will produce an error. NumPy only can be processed at CPU level\n",
        "  z = z.to(\"CPU\") # Move the Tensor to the CPU"
      ],
      "metadata": {
        "id": "NvdqMkiX5-Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd\n",
        "# Gradient (Grad) is important in our model optimization\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "z = y*y*2\n",
        "#z = z.mean()\n",
        "print(z)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.01], dtype=torch.float32)\n",
        "z.backward(v) # dz/dx, Jacobian products\n",
        "print(x.grad)\n",
        "\n",
        "# ----\n",
        "# Stop PyTorch treating the gradient functions and tracking history in \n",
        "# our compational graphs.\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad()\n",
        "\n",
        "a = torch.rand(3, requires_grad=True) # By default is False\n",
        "print(a)\n",
        "\n",
        "a.requires_grad_(False)\n",
        "print(a)\n",
        "\n",
        "# ---\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  # Clear it before the next epoch so the grad value is still right.\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGFyY7Le--Po",
        "outputId": "a5020c01-1372-4d82-a388-3a76b1b40c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3010, 0.5801, 0.3912], requires_grad=True)\n",
            "tensor([2.3010, 2.5801, 2.3912], grad_fn=<AddBackward0>)\n",
            "tensor([10.5891, 13.3141, 11.4357], grad_fn=<MulBackward0>)\n",
            "tensor([ 0.9204, 10.3205,  0.0956])\n",
            "tensor([0.2824, 0.7041, 0.6977], requires_grad=True)\n",
            "tensor([0.2824, 0.7041, 0.6977])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # Initial weight\n",
        "\n",
        "# Forward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss) # Print loss\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### Update weights\n",
        "### next forward and backwards\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRjNyOhNBYlV",
        "outputId": "c83c6cc1-9e3f-438c-9ae3-056585ba273f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Manual way, using NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "# Gradient\n",
        "# MSE = 1/N * (w*X - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoc % 1 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "PbOXUMb6FCq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a5eae5-9e00-4b1b-8caa-28d742810fe3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 1.2, loss = 30.00000000\n",
            "epoch 2: w = 1.68, loss = 4.79999924\n",
            "epoch 3: w = 1.87, loss = 0.76800019\n",
            "epoch 4: w = 1.95, loss = 0.12288000\n",
            "epoch 5: w = 1.98, loss = 0.01966083\n",
            "epoch 6: w = 1.99, loss = 0.00314574\n",
            "epoch 7: w = 2.0, loss = 0.00050331\n",
            "epoch 8: w = 2.0, loss = 0.00008053\n",
            "epoch 9: w = 2.0, loss = 0.00001288\n",
            "epoch 10: w = 2.0, loss = 0.00000206\n",
            "epoch 11: w = 2.0, loss = 0.00000033\n",
            "epoch 12: w = 2.0, loss = 0.00000005\n",
            "epoch 13: w = 2.0, loss = 0.00000001\n",
            "epoch 14: w = 2.0, loss = 0.00000000\n",
            "epoch 15: w = 2.0, loss = 0.00000000\n",
            "epoch 16: w = 2.0, loss = 0.00000000\n",
            "epoch 17: w = 2.0, loss = 0.00000000\n",
            "epoch 18: w = 2.0, loss = 0.00000000\n",
            "epoch 19: w = 2.0, loss = 0.00000000\n",
            "epoch 20: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent using Autograd\n",
        "# ----\n",
        "# - Prediction: PyTorch Model\n",
        "# - Gradient computation: Autograd\n",
        "# - Loss computation: PyTorch Loss\n",
        "# - Parameter updates: PyTorch Optimizer\n",
        "\n",
        "# Tensor way, using PyTorch\n",
        "\n",
        "import torch\n",
        "\n",
        "# Linear regression\n",
        "# f = W * x\n",
        "\n",
        "# f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "print(f'Predicted before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoc % 10 == 0:\n",
        "    print(f'epoch {epoc+1}: w = {w:.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPwbr1oIKn0f",
        "outputId": "98899168-8cdb-4d5b-8e84-49c0e6727ae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted before training: f(5) = 0.000\n",
            "epoch 1: w = 0.3, loss = 30.00000000\n",
            "epoch 11: w = 1.67, loss = 1.16278565\n",
            "epoch 21: w = 1.93, loss = 0.04506890\n",
            "epoch 31: w = 1.99, loss = 0.00174685\n",
            "epoch 41: w = 2.0, loss = 0.00006770\n",
            "epoch 51: w = 2.0, loss = 0.00000262\n",
            "epoch 61: w = 2.0, loss = 0.00000010\n",
            "epoch 71: w = 2.0, loss = 0.00000000\n",
            "epoch 81: w = 2.0, loss = 0.00000000\n",
            "epoch 91: w = 2.0, loss = 0.00000000\n",
            "Predicted after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Pipeline: Model/Loss/Optimizer\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Rows as the number of the samples\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features) # 4 samples with 1 feature\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Predicted before training: f(5) = {model(X_test).item()}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Stochastic Gradient Descent (SGD)\n",
        "# model.parameters contains the weights\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoc in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = bakcward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoc % 100 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoc+1}: w = {w[0][0].item():.3}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Predicted after training: f(5) = {model(X_test).item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0It_9PdtC5",
        "outputId": "e72a266c-7d2e-4181-baa7-b39ba83333ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Predicted before training: f(5) = -3.603912353515625\n",
            "epoch 1: w = -0.318, loss = 55.37839508\n",
            "epoch 101: w = 1.79, loss = 0.06692869\n",
            "epoch 201: w = 1.84, loss = 0.03674300\n",
            "epoch 301: w = 1.88, loss = 0.02017150\n",
            "epoch 401: w = 1.91, loss = 0.01107389\n",
            "epoch 501: w = 1.94, loss = 0.00607944\n",
            "epoch 601: w = 1.95, loss = 0.00333753\n",
            "epoch 701: w = 1.96, loss = 0.00183227\n",
            "epoch 801: w = 1.97, loss = 0.00100590\n",
            "epoch 901: w = 1.98, loss = 0.00055222\n",
            "Predicted after training: f(5) = 9.970149040222168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) prepare data\n",
        "X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
        "Y = Y.view(Y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# 1) model\n",
        "input_size = n_features\n",
        "ouput_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass and loss\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted, Y)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_numpy, Y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "rIhczOVN6zww",
        "outputId": "c1502862-bc59-400d-f2d1-1e2c57c73e9e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4436.9639\n",
            "epoch: 20, loss = 3308.0378\n",
            "epoch: 30, loss = 2491.6191\n",
            "epoch: 40, loss = 1900.5485\n",
            "epoch: 50, loss = 1472.1865\n",
            "epoch: 60, loss = 1161.4496\n",
            "epoch: 70, loss = 935.8409\n",
            "epoch: 80, loss = 771.9070\n",
            "epoch: 90, loss = 652.6998\n",
            "epoch: 100, loss = 565.9565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "#     - forward pass: compute prediction\n",
        "#     - backward pass: gradients\n",
        "#     - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Convert to tensor\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "Y_train = torch.from_numpy(Y_train.astype(np.float32))\n",
        "Y_test = torch.from_numpy(Y_test.astype(np.float32))\n",
        "\n",
        "y_train = Y_train.view(Y_train.shape[0], 1)\n",
        "y_test = Y_test.view(Y_test.shape[0], 1)\n",
        "\n",
        "# 1) model\n",
        "# f = wx + b, sigmoid at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "# BCELoss: Binary Cross Entropy Loss\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forward pass and loss\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # updates\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "  acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'accuracy = {acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I6B7ws99u-S",
        "outputId": "cd1af114-6c1f-4a50-f89b-4eb207a33621"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569 30\n",
            "epoch: 10, loss = 0.6803\n",
            "epoch: 20, loss = 0.5225\n",
            "epoch: 30, loss = 0.4331\n",
            "epoch: 40, loss = 0.3765\n",
            "epoch: 50, loss = 0.3372\n",
            "epoch: 60, loss = 0.3081\n",
            "epoch: 70, loss = 0.2854\n",
            "epoch: 80, loss = 0.2672\n",
            "epoch: 90, loss = 0.2522\n",
            "epoch: 100, loss = 0.2395\n",
            "accuracy = 0.9298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset & DataLoader\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset):\n",
        "    # data loading\n",
        "    xy = np.loadtxt(dataset, delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "    self.x = torch.from_numpy(xy[:, 1:])\n",
        "    self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1\n",
        "    self.n_samples = xy.shape[0]\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    # dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "WINE_DATASET_CSV = 'https://raw.githubusercontent.com/python-engineer/pytorchTutorial/master/data/wine/wine.csv'\n",
        "\n",
        "dataset = WineDataset(WINE_DATASET_CSV)\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples / 4)\n",
        "print(total_samples, n_iterations)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (inputs, labels) in enumerate(dataloader):\n",
        "    # forward backward, update\n",
        "    if (i+1) % 5 == 0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IwDHpccBX_Z",
        "outputId": "e2ca581e-381c-49f2-9b41-099c40744603"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178 45\n",
            "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
            "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Transforms\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset, transform=None):\n",
        "    # data loading\n",
        "    xy = np.loadtxt(dataset, delimiter=\",\", dtype=np.float32, skiprows=1)\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "    self.x = xy[:, 1:]\n",
        "    self.y = xy[:, [0]]\n",
        "\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    # dataset[0]\n",
        "    sample = self.x[index], self.y[index]\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    \n",
        "    return sample\n",
        "  \n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "class ToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MulTransform:\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    inputs, target = sample\n",
        "    inputs *= self.factor\n",
        "    return inputs, target\n",
        "\n",
        "WINE_DATASET_CSV_URL = 'https://raw.githubusercontent.com/python-engineer/pytorchTutorial/master/data/wine/wine.csv'\n",
        "\n",
        "dataset = WineDataset(dataset=WINE_DATASET_CSV_URL, transform=ToTensor())\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
        "dataset = WineDataset(dataset=WINE_DATASET_CSV_URL, transform=composed)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF2dSjnLGqeK",
        "outputId": "c3b7e75e-e7c9-4a86-8763-453069b3e19c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
            "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
            "        4.2600e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax & Cross Entropy\n",
        "# ----\n",
        "# Softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:', outputs)\n",
        "\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "outputs = torch.softmax(x, dim=0)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4lQZxfmLO61",
        "outputId": "30f6deb3-892d-41d5-e6cb-e130a2323748"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax & Cross Entropy\n",
        "# ----\n",
        "# CrossEntropyLoss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3 samples\n",
        "Y = torch.tensor([2, 0, 1])\n",
        "\n",
        "# nsamples x nclasses = 3x3\n",
        "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1],[2.0, 1.0, 0.1],[0.1, 3.0, 0.1]])\n",
        "Y_pred_bad = torch.tensor([[2.1, 2.0, 0.1],[0.1, 1.0, 2.1],[0.1, 3.0, 0.1]])\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good, 1)\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUalGoUiT6PJ",
        "outputId": "f9b4c5d3-76de-4fbf-a4d0-8baeb8fe0a82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3018244206905365\n",
            "1.7338258028030396\n",
            "tensor([2, 0, 1])\n",
            "tensor([0, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Functions\n",
        "# -----------------------\n",
        "# Most popular activation functions\n",
        "# 1. Step functions -> Not used in practice\n",
        "# 2. Sigmoid --> Typically used in binary classification problem\n",
        "# 3. TanH --> Good choice in hidden layers\n",
        "# 4. ReLU --> If you don't know what to use, just use a ReLU for hidden layers\n",
        "# 5. Leaky ReLU --> Improved version of ReLU\n",
        "# 6. Softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# option 1 (create nn modules)\n",
        "class NeuralNet1(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet1, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.linear(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = torch.relu(self.linear1(x))\n",
        "    out = torch.sigmoid(self.linear2(x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "Aj6l7hdoswzs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed-Forward NeuralNet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# device config\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyper parameters\n",
        "input_size = 784 # 28x28, flattened into 1-D Tensor\n",
        "hidden_size = 100\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           download=True, \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor())\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size,\n",
        "                                           shuffle=False)\n",
        "\n",
        "examples = iter(train_loader)\n",
        "samples, labels = examples.next()\n",
        "print(samples.shape, labels.shape)\n",
        "\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.imshow(samples[i][0], cmap='gray')\n",
        "plt.show\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # 100, 1, 28, 28\n",
        "    # 100, 78\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # backwards\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "\n",
        "# test\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for images, labels in test_loader:\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(images)\n",
        "\n",
        "    # value, index\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    n_samples += labels.shape[0]\n",
        "    n_correct += (predictions == labels).sum().item()\n",
        "  \n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'accuracy = {acc}')\n",
        "\n",
        "  # Saving the model\n",
        "  FILE = \"model.pth\"\n",
        "  torch.save(model.state_dict(), FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "QmoAdLi0xRa9",
        "outputId": "8faf0b91-63d2-4384-bc87-b3973e1de07c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
            "epoch 1/2, step 100/600, loss = 0.4852\n",
            "epoch 1/2, step 200/600, loss = 0.2642\n",
            "epoch 1/2, step 300/600, loss = 0.1979\n",
            "epoch 1/2, step 400/600, loss = 0.1997\n",
            "epoch 1/2, step 500/600, loss = 0.2944\n",
            "epoch 1/2, step 600/600, loss = 0.1743\n",
            "epoch 2/2, step 100/600, loss = 0.1159\n",
            "epoch 2/2, step 200/600, loss = 0.2230\n",
            "epoch 2/2, step 300/600, loss = 0.1983\n",
            "epoch 2/2, step 400/600, loss = 0.0844\n",
            "epoch 2/2, step 500/600, loss = 0.2269\n",
            "epoch 2/2, step 600/600, loss = 0.1264\n",
            "accuracy = 9.71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and Loading Models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "### COMPLETE MODEL ###\n",
        "# torch.save(model, PATH)\n",
        "\n",
        "# model class must be defined somewhere\n",
        "# model = torch.load(PATH)\n",
        "# mode.eval()\n",
        "\n",
        "# Recommended way, save only the parameters\n",
        "### STATE DICT ###\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# model must be created again with parameters\n",
        "# model = Model(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()\n"
      ],
      "metadata": {
        "id": "3LaHd9uWzhHf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cgT08lpJ8P8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}