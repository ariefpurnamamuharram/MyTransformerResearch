{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Modeling with NN.TRANSFORMER and TORCHTEXT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariefpurnamamuharram/MyTransformerResearch/blob/master/Language_Modeling_with_NN_TRANSFORMER_and_TORCHTEXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Created 2021-12-15\n",
        "# Install the required modules\n",
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "Vma7EH28_m82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31395e00-8ee4-4769-cc75-28ccdae8ffab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main reference can be found at \n",
        "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
      ],
      "metadata": {
        "id": "SFbQHqJn5PJ6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rMVQQt_O_AL8"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "# ==================\n",
        "# - The nn.TransformerEncoder consists of multiple layers of nn.TransformerEncoderLayer\n",
        "\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, \n",
        "               nlayers: int, dropout: float = 0.5):\n",
        "    super().__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    self.encoder = nn.Embedding(ntoken, d_model)\n",
        "    self.d_model = d_model\n",
        "    self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "  \n",
        "  def init_weights(self) -> None:\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "  def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      src: Tensor, shape [seq_len, batch_size]\n",
        "      src_mask: Tensor, shape [seq_len, seq_len]\n",
        "    \n",
        "    Returns:\n",
        "      output of Tensor of shape [seq_len, batch_size, ntoken]\n",
        "    \"\"\"\n",
        "    src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output\n",
        "  \n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "  \"\"\" Generates an upper-triangular matrix of -inf, with zeros on diag. \"\"\"\n",
        "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PositionalEncoding module injects some information about the relative or absolute position of the tokens in the sequence.\n",
        "# This use sine and cosine functions for positional encoding.\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "  \n",
        "  def forward(self, x: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "    \"\"\"\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "CzsmadqeADGg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This example uses torchtext to generate Wikitext-2 dataset.\n",
        "\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\" Converts raw text into a flat Tensor \"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vicab\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "  \"\"\"\n",
        "  Divides the data into bsz separate squences, removing extra elements that wouldn;t cleanly fit.\n",
        "\n",
        "  Args:\n",
        "    data: Tensor, shape [N]\n",
        "    bsz: int, batch size\n",
        "  \n",
        "  Returns:\n",
        "    Tensor of shape [N // bsz, bsz]\n",
        "  \"\"\"\n",
        "  seq_len = data.size(0) // bsz\n",
        "  data = data[:seq_len * bsz]\n",
        "  data = data.view(bsz, seq_len).t().contiguous()\n",
        "  return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "-Tb99rVrCrBA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to generate input and target sequence\n",
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    source: Tensor, shape [full_seq_len, batch_size]\n",
        "    i: int\n",
        "  \n",
        "  Returns:\n",
        "    tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "    target has shape [seq_len * batch_size]\n",
        "  \"\"\"\n",
        "  seq_len = min(bptt, len(source) - 1 - i)\n",
        "  data = source[i:i+seq_len]\n",
        "  target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "  return data, target"
      ],
      "metadata": {
        "id": "kvqKv65wDxuM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate an instance\n",
        "ntokens = len(vocab) # size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "d_hid = 200 # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # number in nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2 # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "metadata": {
        "id": "Qik9m7YpGjuc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model\n",
        "\n",
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "  model.train() # turn on train mode\n",
        "  total_loss = 0\n",
        "  log_interval = 200\n",
        "  start_time = time.time()\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "  num_batches = len(train_data) // bptt\n",
        "  for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    batch_size = data.size(0)\n",
        "    if batch_size != bptt: # only on last batch\n",
        "      src_mask = src_mask[:batch_size, :batch_size]\n",
        "    output = model(data, src_mask)\n",
        "    loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "      lr = scheduler.get_last_lr()[0]\n",
        "      ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "      cur_loss = total_loss / log_interval\n",
        "      ppl = math.exp(cur_loss)\n",
        "      print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "      total_loss = 0\n",
        "      start_time = time.time()\n",
        "  \n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "  model.eval() # turn on evaluation mode\n",
        "  total_loss = 0\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "  with torch.no_grad():\n",
        "      for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "        data, targets = get_batch(eval_data, i)\n",
        "        batch_size = data.size(0)\n",
        "        if batch_size != bptt:\n",
        "          src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data, src_mask)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "  return total_loss / (len(eval_data) - 1)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs = 1 # Epoch 1 only for testing purpose\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(model)\n",
        "  val_loss = evaluate(model, val_data)\n",
        "  val_ppl = math.exp(val_loss)\n",
        "  elapsed = time.time() - epoch_start_time\n",
        "  print('-' * 89)\n",
        "  print(f'| end of epoch {epoch:3d} | time {elapsed:5.2f}s| valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    best_model = copy.deepcopy(model)\n",
        "  \n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "eAu2t6T3HCw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab91fe33-c724-4fdb-b242-353a029a3637"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 695.43 | loss  8.09 | ppl  3245.88\n",
            "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 691.88 | loss  6.86 | ppl   955.21\n",
            "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 687.60 | loss  6.42 | ppl   613.94\n",
            "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 688.32 | loss  6.29 | ppl   541.11\n",
            "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 674.91 | loss  6.19 | ppl   485.75\n",
            "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 676.33 | loss  6.14 | ppl   465.85\n",
            "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 676.55 | loss  6.11 | ppl   451.28\n",
            "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 667.40 | loss  6.10 | ppl   447.50\n",
            "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 668.91 | loss  6.03 | ppl   415.59\n",
            "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 670.85 | loss  6.01 | ppl   408.96\n",
            "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 681.53 | loss  5.90 | ppl   366.10\n",
            "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 690.77 | loss  5.97 | ppl   390.49\n",
            "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 698.26 | loss  5.96 | ppl   386.27\n",
            "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 705.48 | loss  5.88 | ppl   358.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time 2072.98s| valid loss  5.80 | valid ppl   331.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test dataset\n",
        "\n",
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print(\"=\" * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | test ppl {test_ppl:8.2f}')\n",
        "print(\"=\" * 89)"
      ],
      "metadata": {
        "id": "EPJJreBrHnCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8569e2ab-8f47-45c8-9228-15af6ee41022"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.72 | test ppl   305.39\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VAiNv1bGSS_i"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}